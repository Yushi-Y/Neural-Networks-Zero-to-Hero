# Neural-Networks-Zero-to-Hero

The course "Neural Networks: Zero to Hero" by Andrej Karpathy

1. [micrograd](https://github.com/Yushi-Y/Neural-Networks-Zero-to-Hero/blob/main/micrograd.ipynb): Step-by-step spelled-out explanation of backpropagation and training of neural networks.
2. [makemore-Part1-bigrams](https://github.com/Yushi-Y/Neural-Networks-Zero-to-Hero/blob/main/makemore_part1_bigrams.ipynb): Implement a simple network for bigram character-level language modeling.
3. [makemore-Part2-MLP](https://github.com/Yushi-Y/Neural-Networks-Zero-to-Hero/blob/main/makemore_part2_mlp.ipynb): Implement a multilayer perceptron (MLP) character-level language model.
4. [makemore-Part3-BatchNorm](https://github.com/Yushi-Y/Neural-Networks-Zero-to-Hero/blob/main/makemore_part3_batchnorm.ipynb): Explore the internals of MLPs with multiple layers and examine the statistics of forward pass activations, backward pass gradients, and some pitfalls when they are scaled incorrectly. Introduce Batch Normalization.

Source code:
- [micrograd](https://github.com/karpathy/micrograd)
- [makemore](https://github.com/karpathy/makemore)
